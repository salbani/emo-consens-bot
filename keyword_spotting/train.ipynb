{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a tiny ML to spot \"Hey\" and \"Pepper\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software Requirements\n",
    "- Python 3\n",
    "- packages\n",
    "    - pytorch\n",
    "    - [nnAudio](https://github.com/KinWaiCheuk/nnAudio)\n",
    "    - [AudioLoader](https://github.com/KinWaiCheuk/AudioLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import WeightedRandomSampler,DataLoader, Dataset\n",
    "from AudioLoader.speech.speechcommands import SPEECHCOMMANDS_12C\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchaudio._backend import load as load_audio\n",
    "from nnAudio.features.mel import MelSpectrogram\n",
    "from pytorch_lightning.core import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "import random\n",
    "import os\n",
    "from typing import NamedTuple\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up configuration\n",
    "#device = 'cuda:0'\n",
    "device = 'mps'\n",
    "batch_size= 5\n",
    "max_epochs = 30\n",
    "check_val_every_n_epoch = 2\n",
    "num_sanity_val_steps = 5\n",
    "data_root= './speech_commands/' # Download the data here\n",
    "download_option= False\n",
    "n_mels= 40 \n",
    "input_dim= (n_mels*101)\n",
    "output_dim= 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import speech commands dataset\n",
    "We have generated the \"Hey\", \"Pepper\" data ourselves, but to train the model we need wrong/unknown data as well to teach the model to distinguish core words from unrecognized ones. For this we use the \"speech commands 12 classes dataset\" from google. It has 35 words, where ten of them are labeled as commands by convention and the rest is labeled as unknown, but we will relabel all as unknown.   \n",
    "\n",
    "If we use the new pytorch 2.0+ we need to change the line 238 in speechcommands.py from AudioLoader:\n",
    "```py\n",
    "        # download_url(url, root, hash_value=checksum, hash_type=\"md5\")\n",
    "        download_url(url, archive)\n",
    "```\n",
    "And also line 14:\n",
    "```py\n",
    "    # from torchaudio.datasets.utils import _extract_zip as extract_archive\n",
    "    from torchaudio.datasets.utils import _extract_tar as extract_archive\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the 12 classes speechcommands dataset\n",
    "# _ = SPEECHCOMMANDS_12C(root=data_root,\n",
    "#                               url='speech_commands_v0.02',\n",
    "#                               folder_in_archive='SpeechCommands',\n",
    "#                               download= True,\n",
    "#                               subset= 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'dataset_gen'\n",
    "keywords = ['hey', 'pepper']\n",
    "directories = ['out-noisy', 'out-wav']\n",
    "\n",
    "sc_folder = 'speech_commands/SpeechCommands/speech_commands_v0.02'\n",
    "HASH_DIVIDER = '_nohash_'\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "label2idx = {\n",
    "    'hey': 0,\n",
    "    'pepper': 1,\n",
    "    '_silence_': 2,\n",
    "    '_unknown_': 3\n",
    "}\n",
    "\n",
    "class Datapoint(NamedTuple):\n",
    "    audio: torch.Tensor\n",
    "    sample_rate: int\n",
    "    label: int\n",
    "\n",
    "def _load_list(root, *filenames):\n",
    "    output = []\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(root, filename)\n",
    "        with open(filepath) as fileobj:\n",
    "            output += [os.path.normpath(os.path.join(root, line.strip())) for line in fileobj]\n",
    "    return output\n",
    "\n",
    "\n",
    "class KWS_Dataset(Dataset):\n",
    "    def __init__(self, subset: Literal['training', 'validation'], seed: int | None = None):\n",
    "        assert subset is None or subset in [\"training\", \"validation\"], (\n",
    "            \"When `subset` not None, it must take a value from \"\n",
    "            \"{'training', 'validation'}.\"\n",
    "        )\n",
    "        \n",
    "        files: list[tuple[str, str]] = []\n",
    "        for keyword in keywords:\n",
    "            for directory in directories:\n",
    "                files += [(os.path.join(dataset_folder, directory, keyword, filename), keyword) for filename in os.listdir(os.path.join(dataset_folder, directory, keyword))]\n",
    "\n",
    "        random.seed(seed)\n",
    "        random.shuffle(files)\n",
    "        cutoff = int(len(files) * 0.75)\n",
    "\n",
    "        if subset == \"training\":\n",
    "            files = files[:cutoff]\n",
    "        elif subset == \"validation\":\n",
    "            files = files[cutoff:]\n",
    "\n",
    "        sc_files = [f for f in sorted(str(p) for p in Path(sc_folder).glob('*/*.wav')) if HASH_DIVIDER in f]\n",
    "\n",
    "        include = None\n",
    "        exclude = None\n",
    "        if subset == \"validation\" or subset is None:\n",
    "            include = _load_list(sc_folder, 'validation_list.txt')\n",
    "        if subset == \"training\" or subset is None:\n",
    "            exclude = _load_list(sc_folder, 'validation_list.txt')\n",
    "        \n",
    "        if(exclude is not None):\n",
    "            sc_files = [f for f in sc_files if os.path.normpath(f) not in exclude]\n",
    "        if(include is not None):\n",
    "            sc_files = [f for f in sc_files if os.path.normpath(f) in include]\n",
    "\n",
    "        files += [(f, '_unknown_') for f in sc_files]\n",
    "        \n",
    "        self.dataset: list[Datapoint] = []\n",
    "        for path, label in files:\n",
    "            audio_samples, rate = load_audio(path) # loading audio\n",
    "            # audio_sample (1, len)\n",
    "            \n",
    "            if audio_samples.shape[0] != 1:  # Ensure audio_samples has a single channel\n",
    "                audio_samples = audio_samples.mean(dim=0, keepdim=True)  # Convert to mono if not already\n",
    "\n",
    "            if audio_samples.shape[1] != SAMPLE_RATE:\n",
    "                pad_length = SAMPLE_RATE-audio_samples.shape[1]\n",
    "                audio_samples = nn.functional.pad(audio_samples, (0,pad_length)) # pad the end of the audio until 1 second\n",
    "                # (1, 16000)\n",
    "            self.dataset.append(Datapoint(audio_samples, rate, label2idx[label])) \n",
    "\n",
    "\n",
    "        if subset=='training':\n",
    "            silence_clips_sc = [\n",
    "                'dude_miaowing.wav',\n",
    "                'white_noise.wav',\n",
    "                'exercise_bike.wav',\n",
    "                'doing_the_dishes.wav',\n",
    "                'pink_noise.wav',\n",
    "            ]\n",
    "            silence_clips_gen = [\n",
    "                'bathroom_1.wav',\n",
    "                'crowd_1_quieter.wav',\n",
    "                'fan_1.wav',\n",
    "                'homeoffice_2.wav',\n",
    "                'office_1_quieter.wav',\n",
    "                'static_1.wav',\n",
    "            ]\n",
    "        elif subset=='validation':\n",
    "            silence_clips_sc = [\n",
    "                'running_tap.wav',\n",
    "            ]\n",
    "            silence_clips_gen = [\n",
    "                'cafeteria_1_quieter.wav',\n",
    "                'homeoffice_1_quieter.wav',\n",
    "                'fan_2.wav',\n",
    "            ]\n",
    "        else:\n",
    "            silence_clips_sc = []\n",
    "            silence_clips_gen = []\n",
    "\n",
    "        for i in silence_clips_sc: \n",
    "            audio_samples, rate = load_audio(os.path.join(sc_folder, '_background_noise_', i))\n",
    "            for start in range(0,\n",
    "                            audio_samples.shape[1] - SAMPLE_RATE,\n",
    "                            SAMPLE_RATE//2):\n",
    "                audio_segment = audio_samples[0, start:start + SAMPLE_RATE]\n",
    "                self.dataset.append(Datapoint(audio_segment.unsqueeze(0), rate, label2idx['_silence_']))   \n",
    "\n",
    "        for i in silence_clips_gen: \n",
    "            audio_samples, rate = load_audio(os.path.join(dataset_folder, 'noise', i))\n",
    "            for start in range(0,\n",
    "                            audio_samples.shape[1] - SAMPLE_RATE,\n",
    "                            SAMPLE_RATE//2):\n",
    "                audio_segment = audio_samples[0, start:start + SAMPLE_RATE]\n",
    "                self.dataset.append(Datapoint(audio_segment.unsqueeze(0), rate, label2idx['_silence_']))   \n",
    "        \n",
    "    def __getitem__(self, n: int):\n",
    "        audio, rate, label = self.dataset[n]\n",
    "        return audio, rate, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "train_dataset = KWS_Dataset(subset='training', seed=seed)\n",
    "valid_dataset = KWS_Dataset(subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [1,1,4.6,1/17]\n",
    "sample_weights = [0] * len(train_dataset)\n",
    "for n, (data,rate,label_name) in enumerate(train_dataset):\n",
    "    class_weight = class_weights[label_name]\n",
    "    sample_weights[n] = class_weight\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights),replacement=True)\n",
    "\n",
    "#Data processing\n",
    "def data_processing(data: list[torch.Tensor]):\n",
    "    waveforms = []\n",
    "    labels = []    \n",
    "    for batch in data:\n",
    "        waveforms.append(batch[0].squeeze(0)) #after squeeze => (audio_len) tensor # remove batch dim\n",
    "        labels.append(batch[2])             \n",
    "    waveform_padded = nn.utils.rnn.pad_sequence(waveforms, batch_first=True)     \n",
    "    output_batch = {'waveforms': waveform_padded, \n",
    "             'labels': torch.tensor(labels),\n",
    "             }\n",
    "    \n",
    "    return output_batch\n",
    "\n",
    "# data loading\n",
    "trainloader = DataLoader(train_dataset, collate_fn=lambda x: data_processing(x), batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "validloader = DataLoader(valid_dataset, collate_fn=lambda x: data_processing(x), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommand(LightningModule):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.mel_layer: MelSpectrogram       \n",
    "        self.criterion: nn.CrossEntropyLoss\n",
    "        self.linearlayer: nn.Linear\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, spec = self(batch['waveforms']) \n",
    "        #return outputs [2D] for calculate loss, return spec [3D] for visual\n",
    "        loss = self.criterion(outputs, batch['labels'].long())\n",
    "\n",
    "        acc = sum(outputs.argmax(-1) == batch['labels'])/outputs.shape[0] #batch wise\n",
    "        \n",
    "        self.log('Train/acc', acc, on_step=False, on_epoch=True)\n",
    "        self.log('Train/Loss', loss, on_step=False, on_epoch=True)\n",
    "        #log(graph title, take acc as data, on_step: plot every step, on_epch: plot every epoch)\n",
    "        return loss\n",
    "\n",
    "     \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n",
    "        \n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        with torch.no_grad():\n",
    "            torch.clamp_(self.mel_layer.mel_basis, 0, 1)\n",
    "        #after optimizer step, do clamp function on mel_basis         \n",
    "\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "       \n",
    "        self.log('Validation/Loss', loss, on_step=False, on_epoch=True)                     \n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}      \n",
    "        self.validation_step_outputs.append(output_dict)  \n",
    "        return output_dict\n",
    "\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in self.validation_step_outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        \n",
    "        self.log('Validation/acc', acc, on_step=False, on_epoch=True)    \n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        model_param = []\n",
    "        for name, params in self.named_parameters():\n",
    "            if 'mel_layer.' in name:\n",
    "                pass\n",
    "            else:\n",
    "                model_param.append(params)          \n",
    "\n",
    "        optimizer = optim.SGD(model_param, lr=1e-3, momentum= 0.9, weight_decay= 0.001)\n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT kernels created, time used = 0.0105 seconds\n",
      "STFT filter created, time used = 0.0011 seconds\n",
      "Mel filter created, time used = 0.0011 seconds\n"
     ]
    }
   ],
   "source": [
    "mel_layer = MelSpectrogram(sr=16000, \n",
    "                           n_fft=480,\n",
    "                           win_length=None,\n",
    "                           n_mels=n_mels, \n",
    "                           hop_length=160,\n",
    "                           window='hann',\n",
    "                           center=True,\n",
    "                           pad_mode='reflect',\n",
    "                           power=2.0,\n",
    "                           htk=False,\n",
    "                           fmin=0.0,\n",
    "                           fmax=None,\n",
    "                           norm=1,\n",
    "                           trainable_mel=False,\n",
    "                           trainable_STFT=False,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linearmodel_nnAudio(SpeechCommand):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.mel_layer = mel_layer       \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.linearlayer = nn.Linear(input_dim, output_dim)\n",
    "      \n",
    "    def forward(self, x): \n",
    "        #x: 2D [B, 16000]\n",
    "        spec = self.mel_layer(x)  \n",
    "        #spec: 3D [B, F40, T101]\n",
    "        \n",
    "        spec = torch.log(spec+1e-10)\n",
    "        flatten_spec = torch.flatten(spec, start_dim=1) \n",
    "        #flatten_spec: 2D [B, F*T(40*101)] \n",
    "        #start_dim: flattening start from 1st dimention\n",
    "        \n",
    "        out = self.linearlayer(flatten_spec) \n",
    "        #out: 2D [B,number of class(12)] \n",
    "                               \n",
    "        return out, spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | mel_layer   | MelSpectrogram   | 0     \n",
      "1 | criterion   | CrossEntropyLoss | 0     \n",
      "2 | linearlayer | Linear           | 16.2 K\n",
      "-------------------------------------------------\n",
      "16.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.2 K    Total params\n",
      "0.065     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19757/19757 [01:05<00:00, 303.02it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19757/19757 [01:05<00:00, 303.85it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19757/19757 [01:12<00:00, 271.14it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19757/19757 [01:05<00:00, 302.64it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19757/19757 [01:05<00:00, 302.71it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19757/19757 [01:14<00:00, 266.89it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19757/19757 [01:04<00:00, 304.20it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 19757/19757 [01:04<00:00, 305.94it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 19757/19757 [01:14<00:00, 264.52it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 19757/19757 [01:05<00:00, 303.66it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 19757/19757 [01:04<00:00, 304.54it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 19757/19757 [01:16<00:00, 256.84it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 19757/19757 [01:04<00:00, 304.97it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 19757/19757 [01:03<00:00, 311.83it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 19757/19757 [01:18<00:00, 253.27it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 19757/19757 [01:03<00:00, 310.74it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 19757/19757 [01:04<00:00, 308.69it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 19757/19757 [01:21<00:00, 242.62it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 19757/19757 [01:04<00:00, 306.31it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 19757/19757 [01:04<00:00, 305.55it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 19757/19757 [01:25<00:00, 232.06it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 19757/19757 [01:08<00:00, 289.09it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 19757/19757 [01:07<00:00, 293.29it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 19757/19757 [02:05<00:00, 157.38it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 19757/19757 [01:11<00:00, 277.89it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 19757/19757 [01:11<00:00, 276.78it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 19757/19757 [02:38<00:00, 124.88it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 19757/19757 [01:13<00:00, 268.31it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 19757/19757 [01:13<00:00, 267.99it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 19757/19757 [02:26<00:00, 134.92it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 19757/19757 [01:17<00:00, 255.12it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 19757/19757 [01:14<00:00, 265.00it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 19757/19757 [02:24<00:00, 136.38it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 19757/19757 [01:19<00:00, 248.96it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 19757/19757 [01:14<00:00, 263.60it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 19757/19757 [02:25<00:00, 136.06it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 19757/19757 [01:19<00:00, 248.68it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 19757/19757 [01:14<00:00, 264.54it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 19757/19757 [02:28<00:00, 133.39it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 19757/19757 [01:23<00:00, 236.39it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 19757/19757 [01:19<00:00, 249.25it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 19757/19757 [02:48<00:00, 117.46it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 19757/19757 [01:24<00:00, 232.75it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 19757/19757 [01:19<00:00, 248.92it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 19757/19757 [03:10<00:00, 103.81it/s, v_num=29]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 19757/19757 [03:10<00:00, 103.81it/s, v_num=29]\n"
     ]
    }
   ],
   "source": [
    "model_nnAudo = Linearmodel_nnAudio()\n",
    "model_nnAudo = model_nnAudo.to(device)\n",
    "\n",
    "trainer = Trainer(max_epochs=max_epochs,\n",
    "    check_val_every_n_epoch= check_val_every_n_epoch,\n",
    "    num_sanity_val_steps=num_sanity_val_steps)\n",
    "\n",
    "trainer.fit(model_nnAudo, trainloader, validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan, nan]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'testing/hey.wav'\n",
    "audio_samples, rate = load_audio(path) # loading audio\n",
    "# audio_sample (1, len)\n",
    "\n",
    "if audio_samples.shape[0] != 1:  # Ensure audio_samples has a single channel\n",
    "    audio_samples = audio_samples.mean(dim=0, keepdim=True)  # Convert to mono if not already\n",
    "\n",
    "if audio_samples.shape[1] != SAMPLE_RATE:\n",
    "    pad_length = SAMPLE_RATE-audio_samples.shape[1]\n",
    "    audio_samples = nn.functional.pad(audio_samples, (0,pad_length)) \n",
    "\n",
    "x = model_nnAudo(audio_samples)[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
