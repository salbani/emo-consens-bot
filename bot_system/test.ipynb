{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {\n",
    "    \"a\": 0.12,\n",
    "    \"b\": 0.34,\n",
    "    \"c\": 0.56,\n",
    "    \"d\": 0.78,\n",
    "    \"e\": 0.90\n",
    "}\n",
    "\n",
    "max(x, key=x.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PepperGPT initialized\n",
      "Detecting speech... 1.00\n",
      "No face detected\n",
      "Speech ended... 0.82\n",
      "Speech duration: 1.66\n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "ChatAgent Answered: \n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "Speech too short... 0.00\n",
      "Detecting speech... 0.96\n",
      "Speech ended... 0.82\n",
      "Speech duration: 2.05\n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "ChatAgent Answered: \n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "Speech too short... 0.00\n",
      "Detecting speech... 0.93\n",
      "Speech ended... 0.79\n",
      "Speech duration: 1.92\n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "ChatAgent Answered: \n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "Speech too short... 0.00\n",
      "Detecting speech... 0.93\n",
      "Speech ended... 0.75\n",
      "Speech duration: 1.54\n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "ChatAgent Answered: \n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "Speech too short... 0.00\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "Detecting speech... 0.93\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "No face detected\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "-------------- Disposing --------------\n",
      "Analyzing emotion...\n",
      "Analyzing emotion...\n",
      "No face detected\n",
      "Disposed InputStreamProvider <bot_system.providers.microphone_provider.MicrophoneProvider object at 0x35e7b22a0>!\n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "ChatAgent Answered: \n",
      "Hallo, wie geht es dir?. Ich bin kein chatbot. Ich bin ein Mensch.\n",
      "Disposed InputStreamProvider <bot_system.handlers.speech_recognition_handler.SpeechRecognitionHandler object at 0x3684b7a40>!\n",
      "No face detected\n",
      "Disposed InputStreamProvider <bot_system.providers.webcam_provider.WebcamProvider object at 0x1190b8c50>!\n",
      "Disposed InputStreamProvider <bot_system.handlers.emotion_handler.EmotionHandler object at 0x368207dd0>!\n",
      "-------------- Disposed --------------\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/simonprivat/Workspace/Projects/emo-consens-bot\")\n",
    "\n",
    "from bot_system.pepper_gpt import PepperGPT\n",
    "\n",
    "prompter = PepperGPT(True, True)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        pass\n",
    "except KeyboardInterrupt:\n",
    "    print(\"-------------- Disposing --------------\")\n",
    "    prompter.dispose()\n",
    "    print(\"-------------- Disposed --------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 4/4 [00:00<00:00, 12.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "objs = DeepFace.analyze(\n",
    "  img_path = \"test_images/img1.jpg\", \n",
    "  actions = ['age', 'gender', 'race', 'emotion'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0.93\n",
      "0.92\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.91\n",
      "0.93\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.91\n",
      "0.92\n",
      "0.91\n",
      "0.92\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0\n",
      "0.93\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.99\n",
      "0.97\n",
      "0.98\n",
      "0.98\n",
      "0.99\n",
      "0.98\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0.98\n",
      "0.96\n",
      "0.98\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.92\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.92\n",
      "0.91\n",
      "0.91\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.98\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.99\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.98\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.97\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0.93\n",
      "0.97\n",
      "0.95\n",
      "0\n",
      "0\n",
      "0.97\n",
      "0.97\n",
      "0.93\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.99\n",
      "0.99\n",
      "0.98\n",
      "0.99\n",
      "0.99\n",
      "0.99\n",
      "0\n",
      "0.98\n",
      "0\n",
      "0.98\n",
      "0.97\n",
      "1.0\n",
      "0.99\n",
      "0.99\n",
      "0.98\n",
      "0.99\n",
      "0.99\n",
      "0\n",
      "1.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.93\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.91\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.94\n",
      "0.94\n",
      "0.97\n",
      "0.95\n",
      "0.97\n",
      "0\n",
      "0.95\n",
      "0.95\n",
      "0\n",
      "0.96\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0.94\n",
      "0.93\n",
      "0.92\n",
      "0.94\n",
      "0.98\n",
      "0.92\n",
      "0.95\n",
      "0.99\n",
      "0\n",
      "0.93\n",
      "0.96\n",
      "0.93\n",
      "0.93\n",
      "0.93\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.95\n",
      "0.93\n",
      "0.93\n",
      "0.93\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.93\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.94\n",
      "0\n",
      "0.94\n",
      "0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.97\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.93\n",
      "0.95\n",
      "0.91\n",
      "0.94\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.97\n",
      "0.97\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.97\n",
      "0.97\n",
      "0.95\n",
      "0.98\n",
      "0.98\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.99\n",
      "0.97\n",
      "0.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(haarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcamera\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Convert grayscale frame to RGB format\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cv2.data import haarcascades\n",
    "from deepface.DeepFace import analyze\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    largest_face = max(faces, key=lambda x: x[2] * x[3], default=None)\n",
    "\n",
    "    if largest_face is not None:\n",
    "        x, y, w, h = largest_face\n",
    "        face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "        result = analyze(face_roi, actions=[\"emotion\"], enforce_detection=False)\n",
    "\n",
    "        # Determine the dominant emotion\n",
    "        emotion = result[0][\"dominant_emotion\"]\n",
    "        print(result[0][\"face_confidence\"])\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Define the colors for each emotion\n",
    "        emotion_colors = {\n",
    "            'angry': (0, 0, 255),\n",
    "            'disgust': (0, 255, 0),\n",
    "            'fear': (255, 0, 0),\n",
    "            'happy': (255, 255, 0),\n",
    "            'sad': (0, 255, 255),\n",
    "            'surprise': (255, 0, 255),\n",
    "            'neutral': (128, 128, 128)\n",
    "        }\n",
    "\n",
    "        # Define the position and size of the emotion bars\n",
    "        bar_x = 30\n",
    "        bar_y = 20\n",
    "        bar_width = 150\n",
    "        bar_height = 20\n",
    "        bar_distance = 10\n",
    "\n",
    "        emotions: dict[str, float] = result[0][\"emotion\"]\n",
    "        i = 0\n",
    "        # Draw the emotion bars\n",
    "        for emotion, probability in emotions.items():\n",
    "            # Calculate the height of the bar based on the probability\n",
    "            bar_length = int(probability / 100 * bar_width)\n",
    "            \n",
    "            bar_offset = i * (bar_height + bar_distance)\n",
    "\n",
    "            # Calculate the position of the top-left corner of the bar\n",
    "            bar_top_left = (bar_x, bar_y + bar_offset)\n",
    "\n",
    "            # Calculate the position of the bottom-right corner of the bar\n",
    "            bar_bottom_right = (bar_x + bar_length, bar_y + bar_height + bar_offset)\n",
    "\n",
    "            # Draw the bar\n",
    "            cv2.rectangle(frame, bar_top_left, bar_bottom_right, emotion_colors[emotion], -1)\n",
    "\n",
    "            # Add the emotion label\n",
    "            cv2.putText(frame, emotion, (bar_x + bar_width + 30, bar_bottom_right[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "            i += 1\n",
    "\n",
    "    # for result in results:\n",
    "    #     # Extract the face ROI (Region of Interest)\n",
    "    #     # face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "    #     # Perform emotion analysis on the face ROI\n",
    "    #     # result = analyze(face_roi, actions=[\"emotion\"], enforce_detection=False)\n",
    "    #     x,y,w,h,_,_ = result[\"region\"].values()\n",
    "\n",
    "    #     # Determine the dominant emotion\n",
    "    #     emotion = result[\"dominant_emotion\"]\n",
    "\n",
    "    #     # Draw rectangle around face and label with predicted emotion\n",
    "    #     cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    #     cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Real-time Emotion Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
