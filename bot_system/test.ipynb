{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading facial landmark predictor...\n",
      "[INFO] camera sensor warming up...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_dlib_pybind11.rectangle' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# detect faces in the grayscale frame\u001b[39;00m\n\u001b[1;32m     63\u001b[0m faces \u001b[38;5;241m=\u001b[39m detector(gray, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m largest_face \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# loop over the face detections\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m largest_face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# detect faces in the grayscale frame\u001b[39;00m\n\u001b[1;32m     63\u001b[0m faces \u001b[38;5;241m=\u001b[39m detector(gray, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m largest_face \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(faces, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m x[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# loop over the face detections\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m largest_face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '_dlib_pybind11.rectangle' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python speaking_detection.py --shape-predictor shape_predictor_68_face_landmarks.dat\n",
    "# python speaking_detection.py --shape-predictor shape_predictor_68_face_landmarks.dat --picamera 1\n",
    "\n",
    "# import the necessary packages\n",
    "import face_recognition\n",
    "import face_recognition.api\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import datetime\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_speaking(prev_img, curr_img, debug=False, threshold=500, width=400, height=400):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prev_img:\n",
    "        curr_img:\n",
    "    Returns:\n",
    "        Bool value if a person is speaking or not\n",
    "    \"\"\"\n",
    "    prev_img = cv2.resize(prev_img, (width, height))\n",
    "    curr_img = cv2.resize(curr_img, (width, height))\n",
    "\n",
    "    diff = cv2.absdiff(prev_img, curr_img)\n",
    "    norm = np.sum(diff) / (width*height) * 100\n",
    "    if debug:\n",
    "        print(norm)\n",
    "    return norm > threshold\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# grab the indices of the facial landmarks for mouth\n",
    "m_start, m_end = face_utils.FACIAL_LANDMARKS_IDXS['mouth']\n",
    "\n",
    "# initialize the video stream and allow the cammera sensor to warmup\n",
    "print(\"[INFO] camera sensor warming up...\")\n",
    "camera = cv2.VideoCapture(0)\n",
    "time.sleep(2.0)\n",
    "\n",
    "prev_mouth_img = None\n",
    "i = 0\n",
    "margin = 10\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream, resize it to\n",
    "    # have a maximum width of 400 pixels, and convert it to\n",
    "    # grayscale\n",
    "    ret, frame = camera.read()\n",
    "    frame = imutils.resize(frame, width=800)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces in the grayscale frame\n",
    "    faces = detector(gray, 0)\n",
    "    largest_face = max(faces, key=lambda x: x[2] * x[3] if len(faces) else 0, default=None)\n",
    "\n",
    "    # loop over the face detections\n",
    "    if largest_face is not None:\n",
    "        rect = largest_face\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # face_recognition.face_locations(frame)\n",
    "\n",
    "        mouth_shape = shape[m_start:m_end+1]\n",
    "\n",
    "        leftmost_x = min(x for x, y in mouth_shape) - margin\n",
    "        bottom_y = min(y for x, y in mouth_shape) - margin\n",
    "        rightmost_x = max(x for x, y in mouth_shape) + margin\n",
    "        top_y = max(y for x, y in mouth_shape) + margin\n",
    "\n",
    "        w = rightmost_x - leftmost_x\n",
    "        h = top_y - bottom_y\n",
    "\n",
    "        x = int(leftmost_x - 0.1 * w)\n",
    "        y = int(bottom_y - 0.1 * h)\n",
    "\n",
    "        w = int(1.2 * w)\n",
    "        h = int(1.2 * h)\n",
    "\n",
    "        mouth_img = gray[bottom_y:top_y, leftmost_x:rightmost_x]\n",
    "\n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        # for (x, y) in mouth_shape:\n",
    "            # cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # confer this\n",
    "        # https://github.com/seanexplode/LipReader/blob/master/TrackFaces.c#L68\n",
    "        if prev_mouth_img is None:\n",
    "            prev_mouth_img = mouth_img\n",
    "        if is_speaking(prev_mouth_img, mouth_img, threshold=700,\n",
    "                                debug=True):\n",
    "            print(str(i), \"speaking\")\n",
    "            i += 1\n",
    "\n",
    "        prev_mouth_img = mouth_img\n",
    "        \n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(haarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcamera\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Convert grayscale frame to RGB format\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cv2.data import haarcascades\n",
    "import face_recognition\n",
    "from deepface.DeepFace import extract_faces\n",
    "import numpy as np\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    largest_face = max(faces, key=lambda x: x[2] * x[3], default=None)\n",
    "\n",
    "    if largest_face is not None:\n",
    "        x, y, w, h = largest_face\n",
    "        face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "        face_landmarks_list = face_recognition.face_landmarks(face_roi, model=\"large\")\n",
    "\n",
    "        for face_landmarks in face_landmarks_list:\n",
    "            # Print the location of each facial feature in this image\n",
    "            facial_features = [\n",
    "                'chin',\n",
    "                'left_eyebrow',\n",
    "                'right_eyebrow',\n",
    "                'nose_bridge',\n",
    "                'nose_tip',\n",
    "                'left_eye',\n",
    "                'right_eye',\n",
    "                'top_lip',\n",
    "                'bottom_lip'\n",
    "            ]\n",
    "\n",
    "            # Let's trace out each facial feature in the image with a line!\n",
    "            for facial_feature in facial_features:\n",
    "                cv2.polylines(frame, [np.array(face_landmarks[facial_feature], np.int32) + (x, y)], isClosed=False, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Real-time Emotion Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 4/4 [00:00<00:00, 12.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from gaze_tracking import GazeTracking\n",
    "\n",
    "gaze = GazeTracking()\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = webcam.read()\n",
    "    gaze.refresh(frame)\n",
    "\n",
    "    new_frame = gaze.annotated_frame()\n",
    "    text = \"\"\n",
    "\n",
    "    if gaze.is_right():\n",
    "        text = \"Looking right\"\n",
    "    elif gaze.is_left():\n",
    "        text = \"Looking left\"\n",
    "    elif gaze.is_center():\n",
    "        text = \"Looking center\"\n",
    "\n",
    "    cv2.putText(new_frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n",
    "    cv2.imshow(\"Demo\", new_frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0.93\n",
      "0.92\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.91\n",
      "0.93\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.92\n",
      "0.91\n",
      "0.92\n",
      "0.91\n",
      "0.92\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0\n",
      "0.93\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.99\n",
      "0.97\n",
      "0.98\n",
      "0.98\n",
      "0.99\n",
      "0.98\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0.98\n",
      "0.96\n",
      "0.98\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.92\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.92\n",
      "0.91\n",
      "0.91\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.98\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.99\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.98\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.97\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0.93\n",
      "0.97\n",
      "0.95\n",
      "0\n",
      "0\n",
      "0.97\n",
      "0.97\n",
      "0.93\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.99\n",
      "0.99\n",
      "0.98\n",
      "0.99\n",
      "0.99\n",
      "0.99\n",
      "0\n",
      "0.98\n",
      "0\n",
      "0.98\n",
      "0.97\n",
      "1.0\n",
      "0.99\n",
      "0.99\n",
      "0.98\n",
      "0.99\n",
      "0.99\n",
      "0\n",
      "1.0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.93\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.91\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.94\n",
      "0.94\n",
      "0.97\n",
      "0.95\n",
      "0.97\n",
      "0\n",
      "0.95\n",
      "0.95\n",
      "0\n",
      "0.96\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0.94\n",
      "0.93\n",
      "0.92\n",
      "0.94\n",
      "0.98\n",
      "0.92\n",
      "0.95\n",
      "0.99\n",
      "0\n",
      "0.93\n",
      "0.96\n",
      "0.93\n",
      "0.93\n",
      "0.93\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.95\n",
      "0.93\n",
      "0.93\n",
      "0.93\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.93\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0.94\n",
      "0\n",
      "0.94\n",
      "0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.97\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.93\n",
      "0.95\n",
      "0.91\n",
      "0.94\n",
      "0.92\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.92\n",
      "0.93\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0.97\n",
      "0.97\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.97\n",
      "0.97\n",
      "0.95\n",
      "0.98\n",
      "0.98\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.99\n",
      "0.97\n",
      "0.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mCascadeClassifier(haarcascades \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaarcascade_frontalface_default.xml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcamera\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Convert grayscale frame to RGB format\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cv2.data import haarcascades\n",
    "from deepface.DeepFace import extract_faces\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    largest_face = max(faces, key=lambda x: x[2] * x[3], default=None)\n",
    "\n",
    "    if largest_face is not None:\n",
    "        x, y, w, h = largest_face\n",
    "        face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "        result = analyze(face_roi, actions=[\"emotion\"], enforce_detection=False)\n",
    "\n",
    "        # Determine the dominant emotion\n",
    "        emotion = result[0][\"dominant_emotion\"]\n",
    "        print(result[0][\"face_confidence\"])\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Define the colors for each emotion\n",
    "        emotion_colors = {\n",
    "            'angry': (0, 0, 255),\n",
    "            'disgust': (0, 255, 0),\n",
    "            'fear': (255, 0, 0),\n",
    "            'happy': (255, 255, 0),\n",
    "            'sad': (0, 255, 255),\n",
    "            'surprise': (255, 0, 255),\n",
    "            'neutral': (128, 128, 128)\n",
    "        }\n",
    "\n",
    "        # Define the position and size of the emotion bars\n",
    "        bar_x = 30\n",
    "        bar_y = 20\n",
    "        bar_width = 150\n",
    "        bar_height = 20\n",
    "        bar_distance = 10\n",
    "\n",
    "        emotions: dict[str, float] = result[0][\"emotion\"]\n",
    "        i = 0\n",
    "        # Draw the emotion bars\n",
    "        for emotion, probability in emotions.items():\n",
    "            # Calculate the height of the bar based on the probability\n",
    "            bar_length = int(probability / 100 * bar_width)\n",
    "            \n",
    "            bar_offset = i * (bar_height + bar_distance)\n",
    "\n",
    "            # Calculate the position of the top-left corner of the bar\n",
    "            bar_top_left = (bar_x, bar_y + bar_offset)\n",
    "\n",
    "            # Calculate the position of the bottom-right corner of the bar\n",
    "            bar_bottom_right = (bar_x + bar_length, bar_y + bar_height + bar_offset)\n",
    "\n",
    "            # Draw the bar\n",
    "            cv2.rectangle(frame, bar_top_left, bar_bottom_right, emotion_colors[emotion], -1)\n",
    "\n",
    "            # Add the emotion label\n",
    "            cv2.putText(frame, emotion, (bar_x + bar_width + 30, bar_bottom_right[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "            i += 1\n",
    "\n",
    "    # for result in results:\n",
    "    #     # Extract the face ROI (Region of Interest)\n",
    "    #     # face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "    #     # Perform emotion analysis on the face ROI\n",
    "    #     # result = analyze(face_roi, actions=[\"emotion\"], enforce_detection=False)\n",
    "    #     x,y,w,h,_,_ = result[\"region\"].values()\n",
    "\n",
    "    #     # Determine the dominant emotion\n",
    "    #     emotion = result[\"dominant_emotion\"]\n",
    "\n",
    "    #     # Draw rectangle around face and label with predicted emotion\n",
    "    #     cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    #     cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Real-time Emotion Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
