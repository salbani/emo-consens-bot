{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mutable default <class 'numpy.ndarray'> for field LANDMARKS is not allowed: use default_factory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpygaze\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyGaze, PyGazeRenderer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m camera \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Workspace/Projects/emo-consens-bot/.venv/lib/python3.12/site-packages/pygaze/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpygaze\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyGaze, PyGazeRenderer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgaze_estimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GazeEstimator\n",
      "File \u001b[0;32m~/Workspace/Projects/emo-consens-bot/.venv/lib/python3.12/site-packages/pygaze/pygaze.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rotation\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_3d_face_model\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgaze_estimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GazeEstimator\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcamera\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Camera\n",
      "File \u001b[0;32m~/Workspace/Projects/emo-consens-bot/.venv/lib/python3.12/site-packages/pygaze/utils.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictConfig\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaceModel\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_model_mediapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaceModelMediaPipe\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_3d_face_model\u001b[39m(config: DictConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FaceModel:\n\u001b[1;32m     17\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m FaceModelMediaPipe()\n",
      "File \u001b[0;32m~/Workspace/Projects/emo-consens-bot/.venv/lib/python3.12/site-packages/pygaze/common/__init__.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_parts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaceParts, FacePartsName\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaceModel\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_model_mediapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaceModelMediaPipe\n",
      "File \u001b[0;32m~/Workspace/Projects/emo-consens-bot/.venv/lib/python3.12/site-packages/pygaze/common/face_model_mediapipe.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mface_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaceModel\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;129;43m@dataclasses\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrozen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mFaceModelMediaPipe\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mFaceModel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250;43m\t\u001b[39;49m\u001b[38;5;124;43;03m\"\"\"3D face model for MediaPipe 468 points mark-up.\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43;03m\tIn the camera coordinate system, the X axis points to the right from\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43;03m\tsystem rotated 180 degrees around the Y axis.\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43;03m\t\"\"\"\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mLANDMARKS\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02279539\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01496097\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[43m\t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m\t\t\t\t\t\t\t\t\t \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:1265\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m-> 1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:994\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m    991\u001b[0m         kw_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;66;03m# Otherwise it's a field of some type.\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m         cls_fields\u001b[38;5;241m.\u001b[39mappend(\u001b[43m_get_field\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cls_fields:\n\u001b[1;32m    997\u001b[0m     fields[f\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m f\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py:852\u001b[0m, in \u001b[0;36m_get_field\u001b[0;34m(cls, a_name, a_type, default_kw_only)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;66;03m# For real fields, disallow mutable defaults.  Use unhashable as a proxy\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# indicator for mutability.  Read the __hash__ attribute from the class,\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# not the instance.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_field_type \u001b[38;5;129;01mis\u001b[39;00m _FIELD \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdefault\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__hash__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 852\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmutable default \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f\u001b[38;5;241m.\u001b[39mdefault)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for field \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    853\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed: use default_factory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[0;31mValueError\u001b[0m: mutable default <class 'numpy.ndarray'> for field LANDMARKS is not allowed: use default_factory"
     ]
    }
   ],
   "source": [
    "from pygaze import PyGaze, PyGazeRenderer\n",
    "import cv2\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "ret, frame = camera.read()\n",
    "pg = PyGaze()\n",
    "pgren = PyGazeRenderer()\n",
    "gaze_result = pg.predict(frame)\n",
    "for face in gaze_result:\n",
    "    print(f\"Face bounding box: {face.bbox}\")\n",
    "    pitch, yaw, roll = face.get_head_angles()\n",
    "    g_pitch, g_yaw = face.get_gaze_angles()\n",
    "    print(f\"Face angles: pitch={pitch}, yaw={yaw}, roll={roll}.\")\n",
    "    print(f\"Distance to camera: {face.distance}\")\n",
    "    print(f\"Gaze angles: pitch={g_pitch}, yaw={g_yaw}\")\n",
    "    print(f\"Gaze vector: {face.gaze_vector}\")\n",
    "    print(f\"Looking at camera: {pg.look_at_camera(face)}\")\n",
    "\t\n",
    "    img = pgren.render(frame, face, draw_face_bbox=True, draw_face_landmarks=False, draw_3dface_model=False,draw_head_pose=False, draw_gaze_vector=True)\n",
    "    cv2.imshow(\"Face\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading facial landmark predictor...\n",
      "[INFO] camera sensor warming up...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_dlib_pybind11.rectangle' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# detect faces in the grayscale frame\u001b[39;00m\n\u001b[1;32m     63\u001b[0m faces \u001b[38;5;241m=\u001b[39m detector(gray, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m largest_face \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfaces\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# loop over the face detections\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m largest_face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# detect faces in the grayscale frame\u001b[39;00m\n\u001b[1;32m     63\u001b[0m faces \u001b[38;5;241m=\u001b[39m detector(gray, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m largest_face \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(faces, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m*\u001b[39m x[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# loop over the face detections\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m largest_face \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '_dlib_pybind11.rectangle' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python speaking_detection.py --shape-predictor shape_predictor_68_face_landmarks.dat\n",
    "# python speaking_detection.py --shape-predictor shape_predictor_68_face_landmarks.dat --picamera 1\n",
    "\n",
    "# import the necessary packages\n",
    "import face_recognition\n",
    "import face_recognition.api\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import datetime\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def is_speaking(prev_img, curr_img, debug=False, threshold=500, width=400, height=400):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prev_img:\n",
    "        curr_img:\n",
    "    Returns:\n",
    "        Bool value if a person is speaking or not\n",
    "    \"\"\"\n",
    "    prev_img = cv2.resize(prev_img, (width, height))\n",
    "    curr_img = cv2.resize(curr_img, (width, height))\n",
    "\n",
    "    diff = cv2.absdiff(prev_img, curr_img)\n",
    "    norm = np.sum(diff) / (width*height) * 100\n",
    "    if debug:\n",
    "        print(norm)\n",
    "    return norm > threshold\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "print(\"[INFO] loading facial landmark predictor...\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# grab the indices of the facial landmarks for mouth\n",
    "m_start, m_end = face_utils.FACIAL_LANDMARKS_IDXS['mouth']\n",
    "\n",
    "# initialize the video stream and allow the cammera sensor to warmup\n",
    "print(\"[INFO] camera sensor warming up...\")\n",
    "camera = cv2.VideoCapture(0)\n",
    "time.sleep(2.0)\n",
    "\n",
    "prev_mouth_img = None\n",
    "i = 0\n",
    "margin = 10\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "    # grab the frame from the threaded video stream, resize it to\n",
    "    # have a maximum width of 400 pixels, and convert it to\n",
    "    # grayscale\n",
    "    ret, frame = camera.read()\n",
    "    frame = imutils.resize(frame, width=800)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces in the grayscale frame\n",
    "    faces = detector(gray, 0)\n",
    "    largest_face = max(faces, key=lambda x: x[2] * x[3] if len(faces) else 0, default=None)\n",
    "\n",
    "    # loop over the face detections\n",
    "    if largest_face is not None:\n",
    "        rect = largest_face\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # face_recognition.face_locations(frame)\n",
    "\n",
    "        mouth_shape = shape[m_start:m_end+1]\n",
    "\n",
    "        leftmost_x = min(x for x, y in mouth_shape) - margin\n",
    "        bottom_y = min(y for x, y in mouth_shape) - margin\n",
    "        rightmost_x = max(x for x, y in mouth_shape) + margin\n",
    "        top_y = max(y for x, y in mouth_shape) + margin\n",
    "\n",
    "        w = rightmost_x - leftmost_x\n",
    "        h = top_y - bottom_y\n",
    "\n",
    "        x = int(leftmost_x - 0.1 * w)\n",
    "        y = int(bottom_y - 0.1 * h)\n",
    "\n",
    "        w = int(1.2 * w)\n",
    "        h = int(1.2 * h)\n",
    "\n",
    "        mouth_img = gray[bottom_y:top_y, leftmost_x:rightmost_x]\n",
    "\n",
    "        # loop over the (x, y)-coordinates for the facial landmarks\n",
    "        # and draw them on the image\n",
    "        # for (x, y) in mouth_shape:\n",
    "            # cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # confer this\n",
    "        # https://github.com/seanexplode/LipReader/blob/master/TrackFaces.c#L68\n",
    "        if prev_mouth_img is None:\n",
    "            prev_mouth_img = mouth_img\n",
    "        if is_speaking(prev_mouth_img, mouth_img, threshold=700,\n",
    "                                debug=True):\n",
    "            print(str(i), \"speaking\")\n",
    "            i += 1\n",
    "\n",
    "        prev_mouth_img = mouth_img\n",
    "        \n",
    "    # show the frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Display the resulting frame\u001b[39;00m\n\u001b[1;32m     50\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReal-time Emotion Detection\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cv2.data import haarcascades\n",
    "import face_recognition\n",
    "from deepface.DeepFace import extract_faces\n",
    "import numpy as np\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    largest_face = max(faces, key=lambda x: x[2] * x[3], default=None)\n",
    "\n",
    "    if largest_face is not None:\n",
    "        x, y, w, h = largest_face\n",
    "        face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "        face_landmarks_list = face_recognition.face_landmarks(face_roi, model=\"large\")\n",
    "\n",
    "        for face_landmarks in face_landmarks_list:\n",
    "            # Print the location of each facial feature in this image\n",
    "            facial_features = [\n",
    "                'chin',\n",
    "                'left_eyebrow',\n",
    "                'right_eyebrow',\n",
    "                'nose_bridge',\n",
    "                'nose_tip',\n",
    "                'left_eye',\n",
    "                'right_eye',\n",
    "                'top_lip',\n",
    "                'bottom_lip'\n",
    "            ]\n",
    "\n",
    "            # Let's trace out each facial feature in the image with a line!\n",
    "            for facial_feature in facial_features:\n",
    "                cv2.polylines(frame, [np.array(face_landmarks[facial_feature], np.int32) + (x, y)], isClosed=False, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Real-time Emotion Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 4/4 [00:00<00:00, 12.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from gaze_tracking import GazeTracking\n",
    "\n",
    "gaze = GazeTracking()\n",
    "webcam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    _, frame = webcam.read()\n",
    "    gaze.refresh(frame)\n",
    "\n",
    "    new_frame = gaze.annotated_frame()\n",
    "    text = \"\"\n",
    "\n",
    "    if gaze.is_right():\n",
    "        text = \"Looking right\"\n",
    "    elif gaze.is_left():\n",
    "        text = \"Looking left\"\n",
    "    elif gaze.is_center():\n",
    "        text = \"Looking center\"\n",
    "\n",
    "    cv2.putText(new_frame, text, (60, 60), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 0, 0), 2)\n",
    "    cv2.imshow(\"Demo\", new_frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.97\n",
      "0.96\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0\n",
      "0.96\n",
      "0\n",
      "0\n",
      "0\n",
      "0.97\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0.96\n",
      "0.98\n",
      "0\n",
      "0.98\n",
      "0.97\n",
      "0.97\n",
      "0.98\n",
      "0.96\n",
      "0.93\n",
      "0.95\n",
      "0.95\n",
      "0.97\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.95\n",
      "0.96\n",
      "0.91\n",
      "0.92\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0\n",
      "0.93\n",
      "0.91\n",
      "0.93\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "0.9\n",
      "0.89\n",
      "0.93\n",
      "0.93\n",
      "0.92\n",
      "0\n",
      "0.94\n",
      "0.93\n",
      "0\n",
      "0\n",
      "0.94\n",
      "0\n",
      "0.94\n",
      "0.93\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.95\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.93\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.93\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.93\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.94\n",
      "0.93\n",
      "0.93\n",
      "0.95\n",
      "0.93\n",
      "0.93\n",
      "0.91\n",
      "0.93\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0\n",
      "0.96\n",
      "0.94\n",
      "0.93\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.93\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.95\n",
      "0.96\n",
      "0.94\n",
      "0.94\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.97\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.96\n",
      "0.95\n",
      "0.92\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.93\n",
      "0.95\n",
      "0.95\n",
      "0.93\n",
      "0.91\n",
      "0.95\n",
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0.97\n",
      "0.94\n",
      "0.95\n",
      "0.94\n",
      "0.95\n",
      "0.97\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.95\n",
      "0.97\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.95\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0.95\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.96\n",
      "0.94\n",
      "0.96\n",
      "0.93\n",
      "0.93\n",
      "0.96\n",
      "0.95\n",
      "0.97\n",
      "0.97\n",
      "0.96\n",
      "0.97\n",
      "0.96\n",
      "0.96\n",
      "0.96\n",
      "0.97\n",
      "0.97\n",
      "0.95\n",
      "0.94\n",
      "0.93\n",
      "0.96\n",
      "0.95\n",
      "0.95\n",
      "0.94\n",
      "0.96\n",
      "0.97\n",
      "0.98\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cv2.data import haarcascades\n",
    "from deepface.DeepFace import analyze\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "face_cascade = cv2.CascadeClassifier(haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    largest_face = max(faces, key=lambda x: x[2] * x[3], default=None)\n",
    "\n",
    "    if largest_face is not None:\n",
    "        x, y, w, h = largest_face\n",
    "        face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "        result = analyze(face_roi, actions=[\"emotion\"], enforce_detection=False)\n",
    "\n",
    "        # Determine the dominant emotion\n",
    "        emotion = result[0][\"dominant_emotion\"]\n",
    "        print(result[0][\"face_confidence\"])\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Define the colors for each emotion\n",
    "        emotion_colors = {\n",
    "            'angry': (0, 0, 255),\n",
    "            'disgust': (0, 255, 0),\n",
    "            'fear': (255, 0, 0),\n",
    "            'happy': (255, 255, 0),\n",
    "            'sad': (0, 255, 255),\n",
    "            'surprise': (255, 0, 255),\n",
    "            'neutral': (128, 128, 128)\n",
    "        }\n",
    "\n",
    "        # Define the position and size of the emotion bars\n",
    "        bar_x = 30\n",
    "        bar_y = 20\n",
    "        bar_width = 150\n",
    "        bar_height = 20\n",
    "        bar_distance = 10\n",
    "\n",
    "        emotions: dict[str, float] = result[0][\"emotion\"]\n",
    "        i = 0\n",
    "        # Draw the emotion bars\n",
    "        for emotion, probability in emotions.items():\n",
    "            # Calculate the height of the bar based on the probability\n",
    "            bar_length = int(probability / 100 * bar_width)\n",
    "            \n",
    "            bar_offset = i * (bar_height + bar_distance)\n",
    "\n",
    "            # Calculate the position of the top-left corner of the bar\n",
    "            bar_top_left = (bar_x, bar_y + bar_offset)\n",
    "\n",
    "            # Calculate the position of the bottom-right corner of the bar\n",
    "            bar_bottom_right = (bar_x + bar_length, bar_y + bar_height + bar_offset)\n",
    "\n",
    "            # Draw the bar\n",
    "            cv2.rectangle(frame, bar_top_left, bar_bottom_right, emotion_colors[emotion], -1)\n",
    "\n",
    "            # Add the emotion label\n",
    "            cv2.putText(frame, emotion, (bar_x + bar_width + 30, bar_bottom_right[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "            i += 1\n",
    "\n",
    "    # for result in results:\n",
    "    #     # Extract the face ROI (Region of Interest)\n",
    "    #     # face_roi = rgb_frame[y : y + h, x : x + w]\n",
    "\n",
    "    #     # Perform emotion analysis on the face ROI\n",
    "    #     # result = analyze(face_roi, actions=[\"emotion\"], enforce_detection=False)\n",
    "    #     x,y,w,h,_,_ = result[\"region\"].values()\n",
    "\n",
    "    #     # Determine the dominant emotion\n",
    "    #     emotion = result[\"dominant_emotion\"]\n",
    "\n",
    "    #     # Draw rectangle around face and label with predicted emotion\n",
    "    #     cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    #     cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Real-time Emotion Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import gaze\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh # initialize the face mesh model\n",
    "\n",
    "# camera stream:\n",
    "cap = cv2.VideoCapture(1)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,                            # number of faces to track in each frame\n",
    "        refine_landmarks=True,                      # includes iris landmarks in the face mesh model\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:                            # no frame input\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # frame to RGB for the face-mesh model\n",
    "        results = face_mesh.process(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            gaze.gaze(image, results.multi_face_landmarks[0])\n",
    "\n",
    "        cv2.imshow('output window', image)\n",
    "        if cv2.waitKey(2) & 0xFF == 27:          \n",
    "            break\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp-plot.html'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Define the 3D coordinates\n",
    "coords = [\n",
    "    (0.0, 0.0, 0.0),  # Nose tip\n",
    "    (0, -63.6, -12.5),  # Chin\n",
    "    (-43.3, 32.7, -26),  # Left eye, left corner\n",
    "    (43.3, 32.7, -26),  # Right eye, right corner\n",
    "    (-28.9, -28.9, -24.1),  # Left Mouth corner\n",
    "    (28.9, -28.9, -24.1),  # Right mouth corner\n",
    "    (-29.05, 32.7, -39.5),\n",
    "    (29.05, 32.7, -39.5),\n",
    "]\n",
    "\n",
    "# Separate the coordinates into x, y, and z lists\n",
    "x, y, z = zip(*coords)\n",
    "\n",
    "# Create scatter points for specific coordinates\n",
    "scatter = go.Scatter3d(x=x, y=y, z=z, mode=\"markers+text\", marker=dict(color=\"red\", size=5), text=[f\"({xi}, {yi}, {zi})\" for xi, yi, zi in coords], textposition=\"top center\")\n",
    "\n",
    "# Layout settings\n",
    "layout = go.Layout(title=\"3D Coordinates Plot\", scene=dict(xaxis_title=\"X axis\", yaxis_title=\"Y axis\", zaxis_title=\"Z axis\"))\n",
    "\n",
    "# Create a figure\n",
    "fig = go.Figure(data=[scatter], layout=layout)\n",
    "\n",
    "# Show the plot in the browser\n",
    "pyo.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points: [10 12 11 14 13 15 14 16 15 17]\n",
      "Absolute changes: [ 2 -1  3 -1  2 -1  2 -1  2]\n",
      "Relative changes: [ 0.2        -0.08333333  0.27272727 -0.07142857  0.15384615 -0.06666667\n",
      "  0.14285714 -0.0625      0.13333333]\n",
      "Standard deviation of fluctuations: 0.1308438971205142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example scalar data points\n",
    "data_points = np.array([10, 12, 11, 14, 13, 15, 14, 16, 15, 17])\n",
    "\n",
    "# Calculate absolute changes between consecutive data points\n",
    "absolute_changes = np.diff(data_points)\n",
    "\n",
    "# Calculate relative changes (relative to the previous data point)\n",
    "relative_changes = absolute_changes / data_points[:-1]\n",
    "\n",
    "# Compute the standard deviation of the relative changes\n",
    "std_dev_fluctuations = np.std(relative_changes)\n",
    "\n",
    "# Print the results\n",
    "print(\"Data points:\", data_points)\n",
    "print(\"Absolute changes:\", absolute_changes)\n",
    "print(\"Relative changes:\", relative_changes)\n",
    "print(\"Standard deviation of fluctuations:\", std_dev_fluctuations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data points: [10 12 11 14 13 15 14 16 15 17]\n",
      "Absolute changes: [ 2 -1  3 -1  2 -1  2 -1  2]\n",
      "Relative changes: [ 0.2        -0.08333333  0.27272727 -0.07142857  0.15384615 -0.06666667\n",
      "  0.14285714 -0.0625      0.13333333]\n",
      "Fluctuations: [0.2        0.08333333 0.27272727 0.07142857 0.15384615 0.06666667\n",
      " 0.14285714 0.0625     0.13333333]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17857143"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example scalar data points\n",
    "data_points = np.array([10, 12, 11, 14, 13, 15, 14, 16, 15, 17])\n",
    "\n",
    "# Calculate absolute changes between consecutive data points\n",
    "absolute_changes = np.diff(data_points)\n",
    "\n",
    "# Calculate relative changes (relative to the previous data point)\n",
    "relative_changes = absolute_changes / data_points[:-1]\n",
    "\n",
    "# Compute the fluctuations\n",
    "fluctuations = np.abs(relative_changes)\n",
    "\n",
    "# Print the results\n",
    "print(\"Data points:\", data_points)\n",
    "print(\"Absolute changes:\", absolute_changes)\n",
    "print(\"Relative changes:\", relative_changes)\n",
    "print(\"Fluctuations:\", fluctuations)\n",
    "\n",
    "np.mean([0.17857143])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [5],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord = (4, 5,)\n",
    "\n",
    "np.concatenate([coord[:3], [0, 0, 0, 1][min(len(coord), 3):]]).reshape(-1, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
